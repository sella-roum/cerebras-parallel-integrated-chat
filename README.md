# Cerebras Parallel Integrated Chat (Dynamic Parallel Agent Architecture)

これは、Next.js 16 (App Router) と Cerebras AI SDK を使用して構築された、**動的並列エージェント型**のチャットアプリケーションです。

従来の「単一のAIと話す」形式とも、「事前に役割を固定したエージェント群」とも異なり、**ユーザーの問いに合わせて動的に役割や指示を生成し、登録されているすべての高速モデルを一斉に並列稼働させて、その結果を統合（Integration）する**というアーキテクチャを採用しています。

---

## 1\. 主な機能と特徴

### 🧠 動的エージェント・モード (Dynamic Agent Modes)

ユーザーは目的に応じて、以下の9つの「思考モード」を選択できます。システムは選択されたモードに従って、全推論モデルのリソースを動的に最適化します。

- **標準モード (Standard):** 全モデルで並列推論を行い、統合モデルがベストな回答を作成します。
- **エキスパート・チーム (Expert Team):**
  - **動的役割生成:** タスク内容に基づき、統合モデルが「必要な専門家の役割（例：セキュリティ専門家、UXデザイナー）」を自動生成します。
  - **全軍出撃:** 生成された役割を、登録済みの全モデルに動的に割り当てて並列実行させます。
- **深層思考 (Deep Thought / CoT):** 全モデルに対し「思考プロセス」と「最終回答」を分けて出力するよう強制し、論理的整合性を重視して統合します。
- **生成と批評 (Critique):** 全モデルが回答案（草稿）を作成し、その後全モデルが「批評家」となって相互レビューを行い、最後に統合します。
- **動的ルーター (Dynamic Router):** 質問に対して「最も効果的なシステム指示（戦略）」を統合モデルが考案し、その指示を全モデルに適用して実行します。
- **階層型マネージャー (Manager):** タスクをサブタスクに分解し、並列実行してレポートをまとめます。
- **自己反省ループ (Reflection Loop):** 生成 → 批評 → 改訂のループを内部で回し、品質を高めます。
- **投機的実行 (Hypothesis):** 曖昧な質問に対して複数の解釈（仮説）を立てて検証します。
- **感情・トーン分析 (Emotion Analysis):** ユーザーの感情を分析し、最適なトーン（共感的、論理的など）で回答を生成します。

### 🚀 その他の機能

- **ローカル・ファースト:** 会話履歴、設定、APIキーはすべてブラウザの **IndexedDB** に保存されます。サーバーサイドDBは不要です。
- **コンテキスト自動圧縮:** 会話が長くなると「要約モデル」が自動で履歴を圧縮し、トークンあふれを防ぎます。
- **堅牢なストリーミング:** 独自のストリームプロトコルにより、思考の途中経過（「計画中...」「批評中...」）をリアルタイムでUIに表示します。
- **メッセージ編集・再生成:** ユーザーメッセージを編集して会話を分岐させたり、AIの回答のみを再生成可能です。

---

## 2\. アーキテクチャ (4層分離)

保守性と拡張性を高めるため、アプリケーションは以下の4層で構成されています。

1.  **UI層 (`components/`)**:
    - ユーザー操作、Markdownレンダリング、IndexedDBとの同期を担当。
    - `chat-view.tsx` がストリームをパースし、UIを更新します。
2.  **API層 (`app/api/`)**:
    - クライアントからのリクエストを受け付け、エージェントの実行計画を進行させます。
    - ステートレスに設計されており、リクエストごとに実行コンテキストを構築します。
3.  **エージェント層 (`lib/agents/`)**:
    - 各思考モードの「実行計画（ステップ）」と「ステップ関数」を定義します。
    - 例: `executeExpertTeam`, `integrateStandard` など。
4.  **LLM基盤層 (`lib/llm-core/`)**:
    - **Parallel Executor:** 複数のモデルへのリクエストを並列処理し、エラーハンドリング（リトライ）を行います。
    - **Integrator Executor:** 統合モデルや要約モデルの実行を管理します。
    - **API Key Manager:** APIキーのローテーションとレートリミット回避を担当します。

---

## 3\. 設定とカスタマイズ

### 推論モデル (Inference Models)

- Cerebrasなどの高速モデルを複数登録できます。
- **役割 (Role) 設定について:**
  - 以前のバージョンではこの設定で実行モデルをフィルタリングしていましたが、現在は\*\*「ユーザーからの希望（ヒント）」\*\*として扱われます。
  - エキスパートモード等では、ここで入力された役割を参考にしつつ、タスクに合わせて最適な役割が動的に全モデルへ割り振られます。

### 統合モデル (Integrator Model)

- 並列実行された複数の回答をまとめ上げたり、タスクの計画（役割生成、指示生成）を行う「司令塔」となるモデルです。
- 比較的高性能なモデル（Llama-3.3-70bなど）の設定を推奨します。

---

## 4\. セットアップ

1.  **インストール:**

    ```bash
    pnpm install
    ```

2.  **環境変数:**
    `.env.local` にCerebras APIキーを設定します（カンマ区切りで複数可）。

    ```env
    CEREBRAS_API_KEYS=key_1,key_2,key_3
    ```

3.  **起動:**

    ```bash
    pnpm dev
    ```

---

## 5\. 技術スタック

- **Framework:** Next.js 16 (App Router)
- **Language:** TypeScript 5.x
- **UI Library:** React 19, Tailwind CSS 4, shadcn/ui
- **AI Integration:** Vercel AI SDK (Core), Cerebras AI SDK
- **Storage:** IndexedDB (Client-side)
